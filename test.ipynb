{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "685dc168",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhaoxu.li/.conda/envs/EHA/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/projects/_ssd/ZhaoxuCode/Efficient-HA/transformers-4.57.3/src/transformers/utils/hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
      "/projects/_ssd/ZhaoxuCode/Efficient-HA/transformers-4.57.3/src/transformers/models/auto/modeling_auto.py:2284: FutureWarning: The class `AutoModelForVision2Seq` is deprecated and will be removed in v5.0. Please use `AutoModelForImageTextToText` instead.\n",
      "  warnings.warn(\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:09<00:00,  3.30s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "\n",
    "\n",
    "print(\"Loading model...\")# Load model directly\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\",device_map=\"cuda:0\")\n",
    "model = AutoModelForVision2Seq.from_pretrained(\"llava-hf/llava-1.5-7b-hf\",device_map=\"cuda:0\", dtype=\"bfloat16\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4f304960",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"image\", \"image\": \"/projects/_hdd/Datazx/coco_val_images/val2014/COCO_val2014_000000581929.jpg\"},\n",
    "            {\"type\": \"text\", \"text\": \"Describe the image in detail.\"}\n",
    "        ]\n",
    "    },\n",
    "]\n",
    "inputs = processor.apply_chat_template(\n",
    "\tmessages,\n",
    "\tadd_generation_prompt=True,\n",
    "\ttokenize=True,\n",
    "\treturn_dict=True,\n",
    "\treturn_tensors=\"pt\",\n",
    ").to(model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e08917",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 595])\n"
     ]
    }
   ],
   "source": [
    "print(inputs['input_ids'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df4703f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using our method\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lm_head = model.lm_head\n",
    "norm = model.language_model.norm\n",
    "model.set_attn_implementation('eager')\n",
    "position = {\n",
    "                            \"image_start\": inputs['input_ids'].tolist()[0].index(32000), \n",
    "                            \"image_end\": inputs['input_ids'].tolist()[0].index(32000)+24*24, \n",
    "                        }\n",
    "outputs = model.generate(**inputs, max_new_tokens=512,  do_sample=False, use_ours=True,alpha=0.6, threshold_top_p=0.9, threshold_top_k=20,\n",
    "                                        early_exit_layers=[i for i in range(18,26)], lm_head=lm_head,\n",
    "                norm=norm, position=position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "813d517e",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_ids_trimmed = [\n",
    "        out_ids[len(in_ids):] for in_ids, out_ids in zip(inputs.input_ids, outputs)\n",
    "    ]\n",
    "output_text = processor.batch_decode(\n",
    "        generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "    )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00dac65f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The image depicts a grassy field where two horses are grazing on grass. One horse is positioned towards the left side of the field, while the other horse is located more centrally within the field. Both horses are enjoying their time grazing on the grassy field.\n"
     ]
    }
   ],
   "source": [
    "print(output_text\n",
    "      )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "EHA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
